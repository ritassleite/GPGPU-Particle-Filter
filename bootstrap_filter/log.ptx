//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-25769353
// Cuda compilation tools, release 10.1, V10.1.105
// Based on LLVM 3.4svn
//

.version 6.4
.target sm_30
.address_size 64

	// .globl	_Z12vec_subtractfPKfPfi
.extern .shared .align 4 .b8 shared_xs[];
.const .align 4 .b8 __cudart_i2opi_f[24] = {65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.visible .entry _Z12vec_subtractfPKfPfi(
	.param .f32 _Z12vec_subtractfPKfPfi_param_0,
	.param .u64 _Z12vec_subtractfPKfPfi_param_1,
	.param .u64 _Z12vec_subtractfPKfPfi_param_2,
	.param .u32 _Z12vec_subtractfPKfPfi_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<8>;


	ld.param.f32 	%f1, [_Z12vec_subtractfPKfPfi_param_0];
	ld.param.u64 	%rd3, [_Z12vec_subtractfPKfPfi_param_1];
	ld.param.u64 	%rd4, [_Z12vec_subtractfPKfPfi_param_2];
	ld.param.u32 	%r6, [_Z12vec_subtractfPKfPfi_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r1, %r7, %r8;
	setp.ge.u32	%p1, %r10, %r6;
	@%p1 bra 	BB0_3;

	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;

BB0_2:
	mul.wide.u32 	%rd5, %r10, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f32 	%f2, [%rd6];
	sub.f32 	%f3, %f1, %f2;
	add.s64 	%rd7, %rd2, %rd5;
	st.global.f32 	[%rd7], %f3;
	add.s32 	%r10, %r3, %r10;
	setp.lt.u32	%p2, %r10, %r6;
	@%p2 bra 	BB0_2;

BB0_3:
	ret;
}

	// .globl	_Z14sum_sequentialPfiPKf
.visible .entry _Z14sum_sequentialPfiPKf(
	.param .u64 _Z14sum_sequentialPfiPKf_param_0,
	.param .u32 _Z14sum_sequentialPfiPKf_param_1,
	.param .u64 _Z14sum_sequentialPfiPKf_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<29>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [_Z14sum_sequentialPfiPKf_param_0];
	ld.param.u32 	%r9, [_Z14sum_sequentialPfiPKf_param_1];
	ld.param.u64 	%rd2, [_Z14sum_sequentialPfiPKf_param_2];
	mov.u32 	%r18, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r3, %r18, %r2;
	shl.b32 	%r10, %r3, 1;
	mov.u32 	%r4, %tid.x;
	add.s32 	%r5, %r10, %r4;
	mov.f32 	%f28, 0f00000000;
	setp.ge.s32	%p1, %r5, %r9;
	@%p1 bra 	BB1_2;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.s32 	%rd4, %r5, 4;
	add.s64 	%rd5, %rd3, %rd4;
	add.s32 	%r11, %r5, %r3;
	mul.wide.u32 	%rd6, %r11, 4;
	add.s64 	%rd7, %rd3, %rd6;
	ld.global.f32 	%f4, [%rd7];
	ld.global.f32 	%f5, [%rd5];
	add.f32 	%f28, %f5, %f4;

BB1_2:
	shl.b32 	%r12, %r4, 2;
	mov.u32 	%r13, shared_xs;
	add.s32 	%r6, %r13, %r12;
	st.shared.f32 	[%r6], %f28;
	bar.sync 	0;
	setp.lt.u32	%p2, %r18, 66;
	@%p2 bra 	BB1_6;

BB1_3:
	shr.u32 	%r8, %r18, 1;
	setp.ge.u32	%p3, %r4, %r8;
	@%p3 bra 	BB1_5;

	add.s32 	%r14, %r8, %r4;
	shl.b32 	%r15, %r14, 2;
	add.s32 	%r17, %r13, %r15;
	ld.shared.f32 	%f6, [%r6];
	ld.shared.f32 	%f7, [%r17];
	add.f32 	%f8, %f7, %f6;
	st.shared.f32 	[%r6], %f8;

BB1_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r18, 131;
	mov.u32 	%r18, %r8;
	@%p4 bra 	BB1_3;

BB1_6:
	setp.gt.u32	%p5, %r4, 31;
	@%p5 bra 	BB1_8;

	ld.volatile.shared.f32 	%f9, [%r6];
	ld.volatile.shared.f32 	%f10, [%r6+128];
	add.f32 	%f11, %f10, %f9;
	st.volatile.shared.f32 	[%r6], %f11;
	ld.volatile.shared.f32 	%f12, [%r6];
	ld.volatile.shared.f32 	%f13, [%r6+64];
	add.f32 	%f14, %f13, %f12;
	st.volatile.shared.f32 	[%r6], %f14;
	ld.volatile.shared.f32 	%f15, [%r6];
	ld.volatile.shared.f32 	%f16, [%r6+32];
	add.f32 	%f17, %f16, %f15;
	st.volatile.shared.f32 	[%r6], %f17;
	ld.volatile.shared.f32 	%f18, [%r6];
	ld.volatile.shared.f32 	%f19, [%r6+16];
	add.f32 	%f20, %f19, %f18;
	st.volatile.shared.f32 	[%r6], %f20;
	ld.volatile.shared.f32 	%f21, [%r6];
	ld.volatile.shared.f32 	%f22, [%r6+8];
	add.f32 	%f23, %f22, %f21;
	st.volatile.shared.f32 	[%r6], %f23;
	ld.volatile.shared.f32 	%f24, [%r6];
	ld.volatile.shared.f32 	%f25, [%r6+4];
	add.f32 	%f26, %f25, %f24;
	st.volatile.shared.f32 	[%r6], %f26;

BB1_8:
	setp.ne.s32	%p6, %r4, 0;
	@%p6 bra 	BB1_10;

	ld.shared.f32 	%f27, [shared_xs];
	cvta.to.global.u64 	%rd8, %rd1;
	mul.wide.u32 	%rd9, %r2, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f32 	[%rd10], %f27;

BB1_10:
	ret;
}

	// .globl	_Z7get_maxPKfPfi
.visible .entry _Z7get_maxPKfPfi(
	.param .u64 _Z7get_maxPKfPfi_param_0,
	.param .u64 _Z7get_maxPKfPfi_param_1,
	.param .u32 _Z7get_maxPKfPfi_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<21>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [_Z7get_maxPKfPfi_param_0];
	ld.param.u64 	%rd2, [_Z7get_maxPKfPfi_param_1];
	ld.param.u32 	%r10, [_Z7get_maxPKfPfi_param_2];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r3, %r1, %r2;
	shl.b32 	%r11, %r3, 1;
	mov.u32 	%r4, %tid.x;
	add.s32 	%r5, %r11, %r4;
	setp.lt.s32	%p1, %r5, %r10;
	shl.b32 	%r12, %r4, 2;
	mov.u32 	%r13, shared_xs;
	add.s32 	%r6, %r13, %r12;
	@%p1 bra 	BB2_2;
	bra.uni 	BB2_1;

BB2_2:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r5, 4;
	add.s64 	%rd5, %rd3, %rd4;
	add.s32 	%r15, %r5, %r3;
	mul.wide.u32 	%rd6, %r15, 4;
	add.s64 	%rd7, %rd3, %rd6;
	ld.global.f32 	%f1, [%rd7];
	ld.global.f32 	%f2, [%rd5];
	setp.gt.f32	%p2, %f2, %f1;
	@%p2 bra 	BB2_4;
	bra.uni 	BB2_3;

BB2_4:
	st.shared.f32 	[%r6], %f2;
	bra.uni 	BB2_5;

BB2_1:
	mov.u32 	%r14, -8388626;
	st.shared.u32 	[%r6], %r14;
	bra.uni 	BB2_5;

BB2_3:
	st.shared.f32 	[%r6], %f1;

BB2_5:
	bar.sync 	0;
	shr.u32 	%r20, %r1, 1;
	setp.eq.s32	%p3, %r20, 0;
	@%p3 bra 	BB2_10;

BB2_6:
	setp.ge.u32	%p4, %r4, %r20;
	@%p4 bra 	BB2_9;

	ld.shared.f32 	%f4, [%r6];
	add.s32 	%r16, %r20, %r4;
	shl.b32 	%r17, %r16, 2;
	add.s32 	%r19, %r13, %r17;
	ld.shared.f32 	%f3, [%r19];
	setp.geu.f32	%p5, %f4, %f3;
	@%p5 bra 	BB2_9;

	st.shared.f32 	[%r6], %f3;

BB2_9:
	bar.sync 	0;
	shr.u32 	%r20, %r20, 1;
	setp.ne.s32	%p6, %r20, 0;
	@%p6 bra 	BB2_6;

BB2_10:
	setp.ne.s32	%p7, %r4, 0;
	@%p7 bra 	BB2_12;

	ld.shared.f32 	%f5, [shared_xs];
	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r2, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f32 	[%rd10], %f5;

BB2_12:
	ret;
}

	// .globl	_Z3siniPKfPf
.visible .entry _Z3siniPKfPf(
	.param .u32 _Z3siniPKfPf_param_0,
	.param .u64 _Z3siniPKfPf_param_1,
	.param .u64 _Z3siniPKfPf_param_2
)
{
	.local .align 4 .b8 	__local_depot3[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<15>;
	.reg .f32 	%f<49>;
	.reg .b32 	%r<96>;
	.reg .b64 	%rd<21>;


	mov.u64 	%SPL, __local_depot3;
	ld.param.u32 	%r36, [_Z3siniPKfPf_param_0];
	ld.param.u64 	%rd8, [_Z3siniPKfPf_param_1];
	ld.param.u64 	%rd9, [_Z3siniPKfPf_param_2];
	mov.u32 	%r37, %ctaid.x;
	mov.u32 	%r38, %ntid.x;
	mov.u32 	%r39, %tid.x;
	mad.lo.s32 	%r1, %r37, %r38, %r39;
	setp.ge.u32	%p1, %r1, %r36;
	@%p1 bra 	BB3_25;

	cvta.to.global.u64 	%rd10, %rd8;
	cvt.u64.u32	%rd1, %r1;
	mul.wide.u32 	%rd11, %r1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	add.u64 	%rd2, %SPL, 0;
	ld.global.f32 	%f43, [%rd12];
	abs.f32 	%f19, %f43;
	setp.neu.f32	%p2, %f19, 0f7F800000;
	@%p2 bra 	BB3_3;

	mov.f32 	%f20, 0f00000000;
	mul.rn.f32 	%f43, %f43, %f20;

BB3_3:
	mul.f32 	%f21, %f43, 0f3F22F983;
	cvt.rni.s32.f32	%r95, %f21;
	cvt.rn.f32.s32	%f22, %r95;
	neg.f32 	%f23, %f22;
	mov.f32 	%f24, 0f3FC90FDA;
	fma.rn.f32 	%f25, %f23, %f24, %f43;
	mov.f32 	%f26, 0f33A22168;
	fma.rn.f32 	%f27, %f23, %f26, %f25;
	mov.f32 	%f28, 0f27C234C5;
	fma.rn.f32 	%f44, %f23, %f28, %f27;
	abs.f32 	%f29, %f43;
	setp.leu.f32	%p3, %f29, 0f47CE4780;
	@%p3 bra 	BB3_14;

	mov.b32 	 %r3, %f43;
	shl.b32 	%r42, %r3, 8;
	or.b32  	%r4, %r42, -2147483648;
	mov.u32 	%r87, 0;
	mov.u64 	%rd19, __cudart_i2opi_f;
	mov.u32 	%r86, -6;
	mov.u64 	%rd20, %rd2;

BB3_5:
	.pragma "nounroll";
	ld.const.u32 	%r45, [%rd19];
	// inline asm
	{
	mad.lo.cc.u32   %r43, %r45, %r4, %r87;
	madc.hi.u32     %r87, %r45, %r4,  0;
	}
	// inline asm
	st.local.u32 	[%rd20], %r43;
	add.s64 	%rd20, %rd20, 4;
	add.s64 	%rd19, %rd19, 4;
	add.s32 	%r86, %r86, 1;
	setp.ne.s32	%p4, %r86, 0;
	@%p4 bra 	BB3_5;

	bfe.u32 	%r48, %r3, 23, 8;
	add.s32 	%r49, %r48, -128;
	shr.u32 	%r50, %r49, 5;
	and.b32  	%r9, %r3, -2147483648;
	st.local.u32 	[%rd2+24], %r87;
	bfe.u32 	%r10, %r3, 23, 5;
	mov.u32 	%r51, 6;
	sub.s32 	%r52, %r51, %r50;
	mul.wide.s32 	%rd15, %r52, 4;
	add.s64 	%rd7, %rd2, %rd15;
	ld.local.u32 	%r88, [%rd7];
	ld.local.u32 	%r89, [%rd7+-4];
	setp.eq.s32	%p5, %r10, 0;
	@%p5 bra 	BB3_8;

	mov.u32 	%r53, 32;
	sub.s32 	%r54, %r53, %r10;
	shr.u32 	%r55, %r89, %r54;
	shl.b32 	%r56, %r88, %r10;
	add.s32 	%r88, %r55, %r56;
	ld.local.u32 	%r57, [%rd7+-8];
	shr.u32 	%r58, %r57, %r54;
	shl.b32 	%r59, %r89, %r10;
	add.s32 	%r89, %r58, %r59;

BB3_8:
	shr.u32 	%r60, %r89, 30;
	shl.b32 	%r61, %r88, 2;
	add.s32 	%r90, %r60, %r61;
	shl.b32 	%r18, %r89, 2;
	shr.u32 	%r62, %r90, 31;
	shr.u32 	%r63, %r88, 30;
	add.s32 	%r19, %r62, %r63;
	setp.eq.s32	%p6, %r62, 0;
	@%p6 bra 	BB3_9;

	not.b32 	%r64, %r90;
	neg.s32 	%r92, %r18;
	setp.eq.s32	%p7, %r18, 0;
	selp.u32	%r65, 1, 0, %p7;
	add.s32 	%r90, %r65, %r64;
	xor.b32  	%r91, %r9, -2147483648;
	bra.uni 	BB3_11;

BB3_9:
	mov.u32 	%r91, %r9;
	mov.u32 	%r92, %r18;

BB3_11:
	clz.b32 	%r94, %r90;
	setp.eq.s32	%p8, %r94, 0;
	shl.b32 	%r66, %r90, %r94;
	mov.u32 	%r67, 32;
	sub.s32 	%r68, %r67, %r94;
	shr.u32 	%r69, %r92, %r68;
	add.s32 	%r70, %r69, %r66;
	selp.b32	%r27, %r90, %r70, %p8;
	mov.u32 	%r71, -921707870;
	mul.hi.u32 	%r93, %r27, %r71;
	setp.eq.s32	%p9, %r9, 0;
	neg.s32 	%r72, %r19;
	selp.b32	%r95, %r19, %r72, %p9;
	setp.lt.s32	%p10, %r93, 1;
	@%p10 bra 	BB3_13;

	mul.lo.s32 	%r73, %r27, -921707870;
	shr.u32 	%r74, %r73, 31;
	shl.b32 	%r75, %r93, 1;
	add.s32 	%r93, %r74, %r75;
	add.s32 	%r94, %r94, 1;

BB3_13:
	mov.u32 	%r76, 126;
	sub.s32 	%r77, %r76, %r94;
	shl.b32 	%r78, %r77, 23;
	add.s32 	%r79, %r93, 1;
	shr.u32 	%r80, %r79, 7;
	add.s32 	%r81, %r80, 1;
	shr.u32 	%r82, %r81, 1;
	add.s32 	%r83, %r82, %r78;
	or.b32  	%r84, %r83, %r91;
	mov.b32 	 %f44, %r84;

BB3_14:
	mul.rn.f32 	%f7, %f44, %f44;
	and.b32  	%r35, %r95, 1;
	setp.eq.s32	%p11, %r35, 0;
	@%p11 bra 	BB3_16;

	mov.f32 	%f30, 0fBAB6061A;
	mov.f32 	%f31, 0f37CCF5CE;
	fma.rn.f32 	%f45, %f31, %f7, %f30;
	bra.uni 	BB3_17;

BB3_16:
	mov.f32 	%f32, 0f3C08839E;
	mov.f32 	%f33, 0fB94CA1F9;
	fma.rn.f32 	%f45, %f33, %f7, %f32;

BB3_17:
	@%p11 bra 	BB3_19;

	mov.f32 	%f34, 0f3D2AAAA5;
	fma.rn.f32 	%f35, %f45, %f7, %f34;
	mov.f32 	%f36, 0fBF000000;
	fma.rn.f32 	%f46, %f35, %f7, %f36;
	bra.uni 	BB3_20;

BB3_19:
	mov.f32 	%f37, 0fBE2AAAA3;
	fma.rn.f32 	%f38, %f45, %f7, %f37;
	mov.f32 	%f39, 0f00000000;
	fma.rn.f32 	%f46, %f38, %f7, %f39;

BB3_20:
	fma.rn.f32 	%f47, %f46, %f44, %f44;
	@%p11 bra 	BB3_22;

	mov.f32 	%f40, 0f3F800000;
	fma.rn.f32 	%f47, %f46, %f7, %f40;

BB3_22:
	and.b32  	%r85, %r95, 2;
	setp.eq.s32	%p14, %r85, 0;
	@%p14 bra 	BB3_24;

	mov.f32 	%f41, 0f00000000;
	mov.f32 	%f42, 0fBF800000;
	fma.rn.f32 	%f47, %f47, %f42, %f41;

BB3_24:
	cvta.to.global.u64 	%rd16, %rd9;
	shl.b64 	%rd17, %rd1, 2;
	add.s64 	%rd18, %rd16, %rd17;
	st.global.f32 	[%rd18], %f47;

BB3_25:
	ret;
}

	// .globl	_Z10log_normalPKfffPfif
.visible .entry _Z10log_normalPKfffPfif(
	.param .u64 _Z10log_normalPKfffPfif_param_0,
	.param .f32 _Z10log_normalPKfffPfif_param_1,
	.param .f32 _Z10log_normalPKfffPfif_param_2,
	.param .u64 _Z10log_normalPKfffPfif_param_3,
	.param .u32 _Z10log_normalPKfffPfif_param_4,
	.param .f32 _Z10log_normalPKfffPfif_param_5
)
{
	.local .align 4 .b8 	__local_depot4[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<18>;
	.reg .f32 	%f<91>;
	.reg .b32 	%r<100>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<21>;


	mov.u64 	%SPL, __local_depot4;
	ld.param.u64 	%rd8, [_Z10log_normalPKfffPfif_param_0];
	ld.param.f32 	%f24, [_Z10log_normalPKfffPfif_param_1];
	ld.param.f32 	%f25, [_Z10log_normalPKfffPfif_param_2];
	ld.param.u64 	%rd9, [_Z10log_normalPKfffPfif_param_3];
	ld.param.u32 	%r36, [_Z10log_normalPKfffPfif_param_4];
	ld.param.f32 	%f26, [_Z10log_normalPKfffPfif_param_5];
	mov.u32 	%r37, %ctaid.x;
	mov.u32 	%r38, %ntid.x;
	mov.u32 	%r39, %tid.x;
	mad.lo.s32 	%r1, %r37, %r38, %r39;
	setp.ge.u32	%p1, %r1, %r36;
	@%p1 bra 	BB4_27;

	setp.lt.f32	%p2, %f25, 0f00800000;
	mul.f32 	%f27, %f25, 0f4B000000;
	selp.f32	%f1, %f27, %f25, %p2;
	selp.f32	%f28, 0fC1B80000, 0f00000000, %p2;
	mov.b32 	 %r40, %f1;
	add.s32 	%r41, %r40, -1059760811;
	and.b32  	%r42, %r41, -8388608;
	sub.s32 	%r43, %r40, %r42;
	mov.b32 	 %f29, %r43;
	cvt.rn.f32.s32	%f30, %r42;
	mov.f32 	%f31, 0f34000000;
	fma.rn.f32 	%f32, %f30, %f31, %f28;
	add.f32 	%f33, %f29, 0fBF800000;
	mov.f32 	%f34, 0f3E1039F6;
	mov.f32 	%f35, 0fBE055027;
	fma.rn.f32 	%f36, %f35, %f33, %f34;
	mov.f32 	%f37, 0fBDF8CDCC;
	fma.rn.f32 	%f38, %f36, %f33, %f37;
	mov.f32 	%f39, 0f3E0F2955;
	fma.rn.f32 	%f40, %f38, %f33, %f39;
	mov.f32 	%f41, 0fBE2AD8B9;
	fma.rn.f32 	%f42, %f40, %f33, %f41;
	mov.f32 	%f43, 0f3E4CED0B;
	fma.rn.f32 	%f44, %f42, %f33, %f43;
	mov.f32 	%f45, 0fBE7FFF22;
	fma.rn.f32 	%f46, %f44, %f33, %f45;
	mov.f32 	%f47, 0f3EAAAA78;
	fma.rn.f32 	%f48, %f46, %f33, %f47;
	mov.f32 	%f49, 0fBF000000;
	fma.rn.f32 	%f50, %f48, %f33, %f49;
	mul.f32 	%f51, %f33, %f50;
	fma.rn.f32 	%f52, %f51, %f33, %f33;
	mov.f32 	%f53, 0f3F317218;
	fma.rn.f32 	%f84, %f32, %f53, %f52;
	setp.lt.u32	%p3, %r40, 2139095040;
	@%p3 bra 	BB4_3;

	mov.f32 	%f54, 0f7F800000;
	fma.rn.f32 	%f84, %f1, %f54, %f54;

BB4_3:
	cvt.u64.u32	%rd1, %r1;
	cvta.to.global.u64 	%rd10, %rd8;
	mul.wide.u32 	%rd11, %r1, 4;
	add.s64 	%rd12, %rd10, %rd11;
	add.u64 	%rd2, %SPL, 0;
	ld.global.f32 	%f85, [%rd12];
	abs.f32 	%f55, %f85;
	setp.neu.f32	%p4, %f55, 0f7F800000;
	@%p4 bra 	BB4_5;

	mov.f32 	%f56, 0f00000000;
	mul.rn.f32 	%f85, %f85, %f56;

BB4_5:
	mul.f32 	%f57, %f85, 0f3F22F983;
	cvt.rni.s32.f32	%r99, %f57;
	cvt.rn.f32.s32	%f58, %r99;
	neg.f32 	%f59, %f58;
	mov.f32 	%f60, 0f3FC90FDA;
	fma.rn.f32 	%f61, %f59, %f60, %f85;
	mov.f32 	%f62, 0f33A22168;
	fma.rn.f32 	%f63, %f59, %f62, %f61;
	mov.f32 	%f64, 0f27C234C5;
	fma.rn.f32 	%f86, %f59, %f64, %f63;
	abs.f32 	%f65, %f85;
	setp.eq.f32	%p5, %f1, 0f00000000;
	cvt.f64.f32	%fd1, %f84;
	fma.rn.f64 	%fd2, %fd1, 0dBFE0000000000000, 0dBFED67F1C864BEB4;
	cvt.rn.f32.f64	%f66, %fd2;
	selp.f32	%f9, 0f7F800000, %f66, %p5;
	setp.leu.f32	%p6, %f65, 0f47CE4780;
	@%p6 bra 	BB4_16;

	mov.b32 	 %r3, %f85;
	shl.b32 	%r46, %r3, 8;
	or.b32  	%r4, %r46, -2147483648;
	mov.u32 	%r91, 0;
	mov.u64 	%rd19, __cudart_i2opi_f;
	mov.u32 	%r90, -6;
	mov.u64 	%rd20, %rd2;

BB4_7:
	.pragma "nounroll";
	ld.const.u32 	%r49, [%rd19];
	// inline asm
	{
	mad.lo.cc.u32   %r47, %r49, %r4, %r91;
	madc.hi.u32     %r91, %r49, %r4,  0;
	}
	// inline asm
	st.local.u32 	[%rd20], %r47;
	add.s64 	%rd20, %rd20, 4;
	add.s64 	%rd19, %rd19, 4;
	add.s32 	%r90, %r90, 1;
	setp.ne.s32	%p7, %r90, 0;
	@%p7 bra 	BB4_7;

	bfe.u32 	%r52, %r3, 23, 8;
	add.s32 	%r53, %r52, -128;
	shr.u32 	%r54, %r53, 5;
	and.b32  	%r9, %r3, -2147483648;
	st.local.u32 	[%rd2+24], %r91;
	bfe.u32 	%r10, %r3, 23, 5;
	mov.u32 	%r55, 6;
	sub.s32 	%r56, %r55, %r54;
	mul.wide.s32 	%rd15, %r56, 4;
	add.s64 	%rd7, %rd2, %rd15;
	ld.local.u32 	%r92, [%rd7];
	ld.local.u32 	%r93, [%rd7+-4];
	setp.eq.s32	%p8, %r10, 0;
	@%p8 bra 	BB4_10;

	mov.u32 	%r57, 32;
	sub.s32 	%r58, %r57, %r10;
	shr.u32 	%r59, %r93, %r58;
	shl.b32 	%r60, %r92, %r10;
	add.s32 	%r92, %r59, %r60;
	ld.local.u32 	%r61, [%rd7+-8];
	shr.u32 	%r62, %r61, %r58;
	shl.b32 	%r63, %r93, %r10;
	add.s32 	%r93, %r62, %r63;

BB4_10:
	shr.u32 	%r64, %r93, 30;
	shl.b32 	%r65, %r92, 2;
	add.s32 	%r94, %r64, %r65;
	shl.b32 	%r18, %r93, 2;
	shr.u32 	%r66, %r94, 31;
	shr.u32 	%r67, %r92, 30;
	add.s32 	%r19, %r66, %r67;
	setp.eq.s32	%p9, %r66, 0;
	@%p9 bra 	BB4_11;

	not.b32 	%r68, %r94;
	neg.s32 	%r96, %r18;
	setp.eq.s32	%p10, %r18, 0;
	selp.u32	%r69, 1, 0, %p10;
	add.s32 	%r94, %r69, %r68;
	xor.b32  	%r95, %r9, -2147483648;
	bra.uni 	BB4_13;

BB4_11:
	mov.u32 	%r95, %r9;
	mov.u32 	%r96, %r18;

BB4_13:
	clz.b32 	%r98, %r94;
	setp.eq.s32	%p11, %r98, 0;
	shl.b32 	%r70, %r94, %r98;
	mov.u32 	%r71, 32;
	sub.s32 	%r72, %r71, %r98;
	shr.u32 	%r73, %r96, %r72;
	add.s32 	%r74, %r73, %r70;
	selp.b32	%r27, %r94, %r74, %p11;
	mov.u32 	%r75, -921707870;
	mul.hi.u32 	%r97, %r27, %r75;
	setp.eq.s32	%p12, %r9, 0;
	neg.s32 	%r76, %r19;
	selp.b32	%r99, %r19, %r76, %p12;
	setp.lt.s32	%p13, %r97, 1;
	@%p13 bra 	BB4_15;

	mul.lo.s32 	%r77, %r27, -921707870;
	shr.u32 	%r78, %r77, 31;
	shl.b32 	%r79, %r97, 1;
	add.s32 	%r97, %r78, %r79;
	add.s32 	%r98, %r98, 1;

BB4_15:
	mov.u32 	%r80, 126;
	sub.s32 	%r81, %r80, %r98;
	shl.b32 	%r82, %r81, 23;
	add.s32 	%r83, %r97, 1;
	shr.u32 	%r84, %r83, 7;
	add.s32 	%r85, %r84, 1;
	shr.u32 	%r86, %r85, 1;
	add.s32 	%r87, %r86, %r82;
	or.b32  	%r88, %r87, %r95;
	mov.b32 	 %f86, %r88;

BB4_16:
	mul.rn.f32 	%f12, %f86, %f86;
	and.b32  	%r35, %r99, 1;
	setp.eq.s32	%p14, %r35, 0;
	@%p14 bra 	BB4_18;

	mov.f32 	%f67, 0fBAB6061A;
	mov.f32 	%f68, 0f37CCF5CE;
	fma.rn.f32 	%f87, %f68, %f12, %f67;
	bra.uni 	BB4_19;

BB4_18:
	mov.f32 	%f69, 0f3C08839E;
	mov.f32 	%f70, 0fB94CA1F9;
	fma.rn.f32 	%f87, %f70, %f12, %f69;

BB4_19:
	@%p14 bra 	BB4_21;

	mov.f32 	%f71, 0f3D2AAAA5;
	fma.rn.f32 	%f72, %f87, %f12, %f71;
	fma.rn.f32 	%f88, %f72, %f12, %f49;
	bra.uni 	BB4_22;

BB4_21:
	mov.f32 	%f74, 0fBE2AAAA3;
	fma.rn.f32 	%f75, %f87, %f12, %f74;
	mov.f32 	%f76, 0f00000000;
	fma.rn.f32 	%f88, %f75, %f12, %f76;

BB4_22:
	fma.rn.f32 	%f89, %f88, %f86, %f86;
	@%p14 bra 	BB4_24;

	mov.f32 	%f77, 0f3F800000;
	fma.rn.f32 	%f89, %f88, %f12, %f77;

BB4_24:
	and.b32  	%r89, %r99, 2;
	setp.eq.s32	%p17, %r89, 0;
	@%p17 bra 	BB4_26;

	mov.f32 	%f78, 0f00000000;
	mov.f32 	%f79, 0fBF800000;
	fma.rn.f32 	%f89, %f89, %f79, %f78;

BB4_26:
	sub.f32 	%f80, %f26, %f89;
	sub.f32 	%f81, %f80, %f24;
	cvt.f64.f32	%fd3, %f81;
	mul.f64 	%fd4, %fd3, 0dBFE0000000000000;
	mul.f64 	%fd5, %fd3, %fd4;
	cvt.f64.f32	%fd6, %f25;
	div.rn.f64 	%fd7, %fd5, %fd6;
	cvt.rn.f32.f64	%f82, %fd7;
	add.f32 	%f83, %f9, %f82;
	cvta.to.global.u64 	%rd16, %rd9;
	shl.b64 	%rd17, %rd1, 2;
	add.s64 	%rd18, %rd16, %rd17;
	st.global.f32 	[%rd18], %f83;

BB4_27:
	ret;
}

	// .globl	_Z15unNormalised_wsPKffPfi
.visible .entry _Z15unNormalised_wsPKffPfi(
	.param .u64 _Z15unNormalised_wsPKffPfi_param_0,
	.param .f32 _Z15unNormalised_wsPKffPfi_param_1,
	.param .u64 _Z15unNormalised_wsPKffPfi_param_2,
	.param .u32 _Z15unNormalised_wsPKffPfi_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<17>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [_Z15unNormalised_wsPKffPfi_param_0];
	ld.param.f32 	%f1, [_Z15unNormalised_wsPKffPfi_param_1];
	ld.param.u64 	%rd2, [_Z15unNormalised_wsPKffPfi_param_2];
	ld.param.u32 	%r2, [_Z15unNormalised_wsPKffPfi_param_3];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.u32	%p1, %r1, %r2;
	@%p1 bra 	BB5_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	sub.f32 	%f3, %f2, %f1;
	mul.f32 	%f4, %f3, 0f3FB8AA3B;
	cvt.rzi.f32.f32	%f5, %f4;
	mov.f32 	%f6, 0fBF317200;
	fma.rn.f32 	%f7, %f5, %f6, %f3;
	mov.f32 	%f8, 0fB5BFBE8E;
	fma.rn.f32 	%f9, %f5, %f8, %f7;
	mul.f32 	%f10, %f9, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f11, %f10;
	add.f32 	%f12, %f5, 0f00000000;
	ex2.approx.f32 	%f13, %f12;
	mul.f32 	%f14, %f11, %f13;
	setp.lt.f32	%p2, %f3, 0fC2D20000;
	selp.f32	%f15, 0f00000000, %f14, %p2;
	setp.gt.f32	%p3, %f3, 0f42D20000;
	selp.f32	%f16, 0f7F800000, %f15, %p3;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f16;

BB5_2:
	ret;
}

	// .globl	_Z12normalise_wsPffi
.visible .entry _Z12normalise_wsPffi(
	.param .u64 _Z12normalise_wsPffi_param_0,
	.param .f32 _Z12normalise_wsPffi_param_1,
	.param .u32 _Z12normalise_wsPffi_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [_Z12normalise_wsPffi_param_0];
	ld.param.f32 	%f1, [_Z12normalise_wsPffi_param_1];
	ld.param.u32 	%r2, [_Z12normalise_wsPffi_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.u32	%p1, %r1, %r2;
	@%p1 bra 	BB6_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.u32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f2, [%rd4];
	div.rn.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd4], %f3;

BB6_2:
	ret;
}


